{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO219m3pUgSEP6OuVaZK5lV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicklausmillican/ISLR_R/blob/main/ISLR_R_Ch6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 6"
      ],
      "metadata": {
        "id": "v06IlZa-LQKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conceptual"
      ],
      "metadata": {
        "id": "VaDsnW3QCYIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1\n",
        "\n",
        "We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain $p + 1$ models, containing $0, 1, 2,...,p$ predictors. Explain your answers:\n",
        "\n",
        "> (a) Which of the three models with $k$ predictors has the smallest *training* RSS?\n",
        "\n",
        "> (b) Which of the three models with $k$ predictors has the smallest *test* RSS?\n",
        "\n",
        "> (c) True or False:\n",
        "\n",
        "> > i. The predictors in the $k$-variable model identified by forward stepwise are a subset of the predictors in the ($k+1$)-variable model identified by forward stepwise selection.\n",
        "\n",
        "> > ii. The predictors in the $k$-variable model identified by backward stepwise are a subset of the predictors in the ($k + 1$)-variable model identified by backward stepwise selection.\n",
        "\n",
        "> > iii. The predictors in the $k$-variable model identified by backward stepwise are a subset of the predictors in the ($k + 1$)-variable model identified by forward stepwise selection.\n",
        "\n",
        "> > iv. The predictors in the $k$-variable model identified by forward stepwise are a subset of the predictors in the ($k+1$)-variable model identified by backward stepwise selection.\n",
        "\n",
        "> > v. The predictors in the $k$-variable model identified by best subset are a subset of the predictors in the ($k + 1$)-variable model identified by best subset selection."
      ],
      "metadata": {
        "id": "jDnvvJi7CcJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answers to a\n",
        "\n",
        "***Which of the three models with $k$ predictors has the smallest <u>training</u> RSS?***\n",
        "\n",
        "Each of the three methods progress, NOT by comparing RSS among models, but rather by comparing an estimate of fit to test data (e.g., AIC, BIC, $R_{adj}^2$).\n",
        "\n",
        "I think this means that any of the three methods may produce the lowest *training* RSS; this would be somewhat incidental since they are not trying to optimize for this."
      ],
      "metadata": {
        "id": "te_xnu53kEEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer to b\n",
        "\n",
        "***Which of the three models with $k$ predictors has the smallest <u>test</u> RSS?***\n",
        "\n",
        "Since only the *best-subset* method will examine all $k$-predictor models, it will discover the best-fit model.  Either stepwise method may also find the same model, but this is not guarenteed."
      ],
      "metadata": {
        "id": "6d4hEYu3k9ey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer to c\n",
        "\n",
        "***True or False:***\n",
        "\n",
        "> ***i. The predictors in the $k$-variable model identified by forward stepwise are a subset of the predictors in the ($k+1$)-variable model identified by forward stepwise selection.***\n",
        "\n",
        "> > TRUE.  Since the $k+1$-variable model is defined as the $k$-variable model plus the next most appropriate ($k+1^{th}$) variable.\n",
        "\n",
        "> >\n",
        "\n",
        "> ***ii. The predictors in the $k$-variable model identified by backward stepwise are a subset of the predictors in the ($k + 1$)-variable model identified by backward stepwise selection.***\n",
        "\n",
        "> > TRUE.  For the analagous reason given in (i).\n",
        "\n",
        "> >\n",
        "\n",
        "> ***iii. The predictors in the $k$-variable model identified by backward stepwise are a subset of the predictors in the ($k + 1$)-variable model identified by forward stepwise selection.***\n",
        "\n",
        "> > FALSE.  These are both \"greedy\" algorithms such that their path though variable-selection space are not necessarily related.\n",
        "\n",
        "> >\n",
        "\n",
        "> ***iv. The predictors in the $k$-variable model identified by forward stepwise are a subset of the predictors in the ($k+1$)-variable model identified by backward stepwise selection.***\n",
        "\n",
        "> > FALSE.  For the analagous reason given in (iii).\n",
        "\n",
        "> >\n",
        "\n",
        "> ***v. The predictors in the $k$-variable model identified by best subset are a subset of the predictors in the ($k + 1$)-variable model identified by best subset selection.***\n",
        "\n",
        "> > FALSE.  Best-subset selection is not a greedy algorithm; all possible models are examined."
      ],
      "metadata": {
        "id": "bAiTdKR7pfVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2\n",
        "\n",
        "For parts (a) through (c), indicate which of i. through iv. is correct. Justify your answer.\n",
        "\n",
        "> (a) The lasso, relative to least squares, is:\n",
        "\n",
        "> > i. More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\n",
        "\n",
        "> > ii. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\n",
        "\n",
        "> > iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\n",
        "\n",
        "> > iv. Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\n",
        "\n",
        "> (b) Repeat (a) for ridge regression relative to least squares.\n",
        "\n",
        "> (c) Repeat (a) for non-linear methods relative to least squares."
      ],
      "metadata": {
        "id": "YnwKwAntZh3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answers\n",
        "\n",
        "The correct answer for both (a) and (b) is *iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance*.  Both lasso and ridge regression impose a penalty term on the fitting equation, which constrains the solution space.  This is akin to imposing a bias, but also tends to reduce variance.  \n",
        "\n",
        "For (c), the correct answer is *ii. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias*.  Nonlinear models are inherently more flexible than linear models.  The expands the solution space, but at the risk of admitting too much variance."
      ],
      "metadata": {
        "id": "5BA9E9WXaF0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPROVE THIS"
      ],
      "metadata": {
        "id": "nCADdH34clrI"
      }
    }
  ]
}